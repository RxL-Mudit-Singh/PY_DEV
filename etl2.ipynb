{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ? insert stat dump\n",
    "import json\n",
    "import cx_Oracle as cx\n",
    "from flatten_json import flatten\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "cnxn_pool = cx.SessionPool('PVD_MART_30AUG','rxlogix','10.100.22.99:1521/PVSDEVDB',min=12,max=15,encoding='UTF-8')\n",
    "\n",
    "def include_keys(dic, keys):\n",
    "    key_set = set(keys) & set(dic.keys())\n",
    "    return {key: dic[key] for key in key_set}\n",
    "\n",
    "def pre_proc(data):\n",
    "        print(type(data))\n",
    "        for k,v in data.items():\n",
    "            if isinstance(v,dict):\n",
    "                if 'ArrayElem' in v.keys():\n",
    "                # for k1,v1 in v.items():\n",
    "                    data[k]=v['ArrayElem']\n",
    "        return data\n",
    "    \n",
    "def flatten_list(d):\n",
    "    try:\n",
    "        key, lst = next((k, v) for k, v in d.items() if isinstance(v, list))\n",
    "    except (StopIteration, AttributeError):\n",
    "        return [flatten(d,'.')]\n",
    "    return [flatten({**d, **{key: v}},'.') for record in lst for v in flatten_list(record)]\n",
    "\n",
    "def generate_inserts(table, data):\n",
    "    jsons = flatten_list(data)\n",
    "    statements = []\n",
    "    ins = jsons[0]\n",
    "    ins = f\"INSERT INTO {table} ({','.join(ins.keys())}) VALUES ({','.join([':'+str(v) for v in range(len(ins.keys()))])})\"\n",
    "    val_array = []\n",
    "    for i in jsons:\n",
    "        cols = i.keys()\n",
    "        values = i.values()\n",
    "        # ins = f\"INSERT INTO {table} ({','.join(cols)}) VALUES ({','.join(['%s']*len(cols))})\"\n",
    "        val_array.append(tuple(values))\n",
    "    statements.append(ins)\n",
    "    statements.append(val_array)\n",
    "    return statements\n",
    "\n",
    "def return_all_inserts(dictionary)->dict:\n",
    "    generic_keys = ['TENANT_ID','CASE_ID','VERSION_NUM']\n",
    "    inserts = {}\n",
    "    dictionary2 = pre_proc(dictionary)\n",
    "    for i in dictionary2:\n",
    "        if i not in generic_keys:\n",
    "            t1 = include_keys(dictionary, generic_keys+[i])\n",
    "            for j in t1:\n",
    "                if isinstance(t1[j],dict):\n",
    "                    if 'ArrayElem' in t1[j].keys():\n",
    "                        t1[j] = t1[j]['ArrayElem']\n",
    "            inserts[i] = tuple(generate_inserts(i,t1))\n",
    "    return inserts\n",
    "\n",
    "\n",
    "def push_data(dictionary1):\n",
    "    print(\"Thread created\")\n",
    "    dictionary2 = return_all_inserts(dictionary1)\n",
    "    cnxn = cnxn_pool.acquire()\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'DD/MM/YYYY hh24:mi:ss'\")\n",
    "    print(\"\"\"INCURSION :)(: STARTED\"\"\")\n",
    "    counter = 1\n",
    "    failed_insert = {}\n",
    "    for key,val  in dictionary2.items(): \n",
    "        val_array = val[1]\n",
    "        # print(val[0],val_array)\n",
    "        try:\n",
    "            cursor.executemany(val[0],val_array)\n",
    "            # cnxn.commit()\n",
    "        except cx.DatabaseError as e:\n",
    "            print(e)\n",
    "            failed_insert[key] = val\n",
    "            # break\n",
    "        # else:\n",
    "            # print(\"Insert Done\")\n",
    "        # print(counter,key)\n",
    "        counter+=1\n",
    "    if len(failed_insert) == 0:\n",
    "        cnxn.commit()\n",
    "        print(\"All Inserts Committed\")\n",
    "    else:\n",
    "        cnxn.rollback()\n",
    "        print(\"Rollback Done\")\n",
    "        # print(failed_insert)\n",
    "    print(\"data chunk inserted\")\n",
    "    cnxn_pool.release(cnxn)\n",
    "    \n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "#     with open('jsons/msg1.json','r') as msg1:\n",
    "#         string = msg1.read()\n",
    "#         D = json.loads(string)\n",
    "#         # dictionary = return_all_inserts(D)\n",
    "#         push_data(D)\n",
    "#     cnxn_pool.close()\n",
    "    \n",
    "def split_file(a,n):\n",
    "    k,m = divmod(len(a),n)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)))\n",
    "\n",
    "def load(file)->list:\n",
    "    with open(file,'r', encoding=\"utf8\") as fp:\n",
    "        string = fp.read()\n",
    "        o = json.loads(string)\n",
    "        line_chunk = split_file(o,1)\n",
    "        print('chunk created')\n",
    "        return line_chunk\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    st = time.time()\n",
    "    chunk_data = load('jsons/msg1.json')\n",
    "    with ThreadPoolExecutor(max_workers=10) as threads:\n",
    "        threads.map(push_data,chunk_data)\n",
    "    cnxn_pool.close()\n",
    "    et = time.time()\n",
    "    print(et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRUNCATING THE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ? trunacting the table\n",
    "import json\n",
    "import cx_Oracle as cx\n",
    "cdns = cx.makedsn('10.100.22.99','1521',service_name='PVSDEVDB')\n",
    "cnxn = cx.connect(user=r'PVD_MART_30AUG', password='rxlogix', dsn=cdns)\n",
    "c = cnxn.cursor()\n",
    "with open('read.json','r') as insert_stat:\n",
    "        string = insert_stat.read()\n",
    "        X = json.loads(string)\n",
    "        arr = []\n",
    "        for key,val  in X.items():\n",
    "            arr.append(key,)\n",
    "            c.execute(f\"TRUNCATE TABLE {key}\")\n",
    "        print(\"truncate complete\")\n",
    "cnxn.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTI-THREADED DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cx_Oracle as cx\n",
    "from flatten_json import flatten\n",
    "import time\n",
    "from multiprocessing import Process,Pool\n",
    "from  threading import  Thread\n",
    "import boto3 as bt\n",
    "import logging\n",
    "import logging_json as lg\n",
    "import jsonlines\n",
    "# %(threadName)s\n",
    "# %(name)s\n",
    "# %(levelname)s\n",
    "# %(asctime)s \n",
    "################################################################################################\n",
    "multikey = []\n",
    "casesProcessed= 0\n",
    "################################################################################################\n",
    "\n",
    "#! Fetching sqs from dump msg\n",
    "def recieve_message(url)->list:\n",
    "    lst = []\n",
    "    sqs_client = bt.client(\"sqs\",region_name=\"ap-south-1\")\n",
    "    response = sqs_client.receive_message(\n",
    "        QueueUrl = url,\n",
    "        MaxNumberOfMessages=3,\n",
    "        WaitTimeSeconds=0\n",
    "    )\n",
    "    val = response['Messages'][0]['Body']\n",
    "    val2 = json.loads(val)\n",
    "    bucket_name = val2['Records'][0]['s3']['bucket']['name']\n",
    "    fileKey = val2['Records'][0]['s3']['object']['key']\n",
    "    lst.append(bucket_name)\n",
    "    lst.append(fileKey)\n",
    "    return lst\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "def include_keys(dic, keys):\n",
    "    key_set = set(keys) & set(dic.keys())\n",
    "    return {key: dic[key] for key in key_set}\n",
    "\n",
    "#! Removing the ArrayElem\n",
    "def pre_proc(data):\n",
    "    blank_key_data = []\n",
    "    for k,v in data.items():\n",
    "        if isinstance(v,dict):\n",
    "            if 'ArrayElem' in v.keys():\n",
    "                data[k]=v['ArrayElem']\n",
    "            if isinstance(data[k],list):\n",
    "                if len(data[k][0]) == 0:\n",
    "                    blank_key_data.append(k)\n",
    "        \n",
    "    # for k1,v1 in data.items():\n",
    "    #     if isinstance(v1,list):\n",
    "    #         if len(v1[0]) == 0:\n",
    "    #             blank_key_data.append(k1)\n",
    "    # {'c_actions_addl':[{}]}\n",
    "    for key_to_be_deleted in blank_key_data:\n",
    "        del data[key_to_be_deleted]\n",
    "    # print(blank_key_data)\n",
    "    # print(data)\n",
    "    return data\n",
    "    \n",
    "    \n",
    "#! Flattening the dictionary \n",
    "def flatten_list(d):\n",
    "    try:\n",
    "        key, lst = next((k, v) for k, v in d.items() if isinstance(v, list))\n",
    "    except (StopIteration, AttributeError):\n",
    "        return [flatten(d,'.')]\n",
    "    return [flatten({**d, **{key: v}},'.') for record in lst for v in flatten_list(record)]\n",
    "\n",
    "def generate_inserts(table, data):\n",
    "    jsons = flatten_list(data)\n",
    "    statements = []\n",
    "    ins = jsons[0]\n",
    "    ins = f\"INSERT INTO {table} ({','.join(ins.keys())}) VALUES ({','.join([':'+str(v) for v in range(len(ins.keys()))])})\"\n",
    "    val_array = []\n",
    "    for i in jsons:\n",
    "        cols = i.keys()\n",
    "        values = i.values()\n",
    "        val_array.append(tuple(values))\n",
    "    statements.append(ins)\n",
    "    statements.append(val_array)\n",
    "    return statements\n",
    "    \n",
    "\n",
    "#! Creating insert statements\n",
    "def return_all_inserts(dictionary)->dict:\n",
    "    generic_keys = ['TENANT_ID','CASE_ID','VERSION_NUM']\n",
    "    inserts = {}\n",
    "    dictionary2 = pre_proc(dictionary)\n",
    "    for i in dictionary2:\n",
    "        if i not in generic_keys:\n",
    "            t1 = include_keys(dictionary, generic_keys+[i])\n",
    "            for j in t1:\n",
    "                if isinstance(t1[j],dict):\n",
    "                    if 'ArrayElem' in t1[j].keys():\n",
    "                        t1[j] = t1[j]['ArrayElem']\n",
    "            inserts[i] = tuple(generate_inserts(i,t1))\n",
    "    with open('read.json','w') as f:\n",
    "        G = json.dumps(inserts)\n",
    "        f.write(G)\n",
    "    return inserts\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "def push_data(dictionary1, cnxn_pool):\n",
    "    global failures\n",
    "    dictionary2 = return_all_inserts(dictionary1)\n",
    "    # print(dictionary2)\n",
    "    st1 = time.time()\n",
    "    cnxn = cnxn_pool.acquire()\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'DD-MM-YYYY hh24:mi:ss'\")\n",
    "    print(\"\"\"INCURSION :)(: STARTED\"\"\")\n",
    "    failed_insert = {}\n",
    "    for key,val  in dictionary2.items(): \n",
    "        val_array = val[1]\n",
    "        print(val[0])\n",
    "        print(val[1])\n",
    "        try:\n",
    "            cursor.executemany(val[0],val[1])\n",
    "        except cx.DatabaseError as e:\n",
    "            multikey.append({\"f_id\" :f\"{dictionary1['TENANT_ID']}^{dictionary1['CASE_ID']}^{dictionary1['VERSION_NUM']}\",\"tables\":f\"{key}-> {e}\"})\n",
    "            failed_insert[key] = val\n",
    "            print(e)\n",
    "        # break\n",
    "        # else:\n",
    "            # print(\"Insert Done\")\n",
    "    if len(failed_insert) == 0:\n",
    "        cnxn.commit()\n",
    "        casesProcessed+=1\n",
    "        print(\"All Inserts Committed\")\n",
    "        \n",
    "    else:\n",
    "        cnxn.rollback()\n",
    "        failures.append(failed_insert)\n",
    "        print(\"Rollback Done\")\n",
    "    cnxn_pool.release(cnxn)\n",
    "    print(f\"Number of cases processed -> : {casesProcessed}\")\n",
    "    \n",
    "################################################################################################\n",
    "\n",
    "# ! Splitting the file into multiple chunk_data \n",
    "def split_file(a,n):\n",
    "    k,m = divmod(len(a),n)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)))\n",
    "\n",
    "def load(file)->list:\n",
    "    with open(file,'r') as fp:\n",
    "        string = fp.read()\n",
    "        o = json.loads(string)\n",
    "        line_chunk = split_file(o,1)\n",
    "        print('chunk created')\n",
    "        return line_chunk\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    st = time.time()\n",
    "    # try:\n",
    "    #     arr = recieve_message(\"https://sqs.ap-south-1.amazonaws.com/884379823401/testQueue\")\n",
    "    #     s3 = bt.client('s3')\n",
    "    #     s3.download_file(Bucket=arr[0],Key=arr[1],Filename='C:/DEV/PY_DEV/output.json')\n",
    "    # except Exception as e:\n",
    "    #     logging.error(\"Error downloading file\")\n",
    "    failures = []\n",
    "    cnxn_pool = cx.SessionPool('PVD_MART_30AUG','rxlogix','10.100.22.99:1521/PVSDEVDB',min=12,max=15,encoding='UTF-8')\n",
    "    chunk_data = load('jsons/msg2.json')\n",
    "    threads = []\n",
    "    for i in chunk_data[0]:\n",
    "        thread = Thread(target=push_data, args=(i,cnxn_pool),daemon=True)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "        \n",
    "    # while True:\n",
    "    #     if any([t.is_alive() for t in threads]):\n",
    "    #         print(\"Threads running\")\n",
    "    #         time.sleep(10)\n",
    "    #     else:\n",
    "    #         break\n",
    "    cnxn_pool.close()\n",
    "    #? Uploading Failed Inserts to S3\n",
    "    # with open('jsons/failues.json', 'w') as F:\n",
    "    #     J = json.dumps(failures, encoding = 'utf-8',indent =1)\n",
    "    #     F.write(J)\n",
    "    #     s3.upload_file(F,Bucket=arr[0])\n",
    "    with open('logs.json','w',encoding='utf-8') as logs:\n",
    "                G = json.dumps(multikey,indent=1)\n",
    "                logs.write(G)\n",
    "    print(f\" length of failures : {len(failures)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def split_file(a,n):\n",
    "    k,m = divmod(len(a),n)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)))\n",
    "\n",
    "def load(file)->list:\n",
    "    with open(file,'r', encoding=\"utf8\") as fp:\n",
    "        string = fp.read()\n",
    "        o = json.loads(string)\n",
    "        line_chunk = split_file(o,1)\n",
    "        print('chunk created')\n",
    "        return line_chunk\n",
    "    \n",
    "chunk_data = load('jsons/msg1.json')\n",
    "print(len(chunk_data[0]))\n",
    "print(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "    \n",
    "\n",
    "with open('1milliondata.json') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    df_inter = pd.DataFrame(lines)\n",
    "    df_inter.columns = ['json_element']\n",
    "    df_inter['json_element'].apply(json.loads)\n",
    "df_inter\n",
    "# print(len(df_inter))\n",
    "# json_obj = pd.read_json('jsons/latestCases.json',lines=True)\n",
    "# print(json_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE INSERT OPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk created\n",
      "INCURSION :)(: STARTED\n",
      "ORA-06550: line 12, column 230:\n",
      "PL/SQL: ORA-00904: \"C_CONTACT_LOG_ADDL\".\"WL_USER_ID\": invalid identifier\n",
      "ORA-06550: line 12, column 1:\n",
      "PL/SQL: SQL Statement ignored\n",
      "Failed or not-> {True}\n",
      " Number of Failed Cases : 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cx_Oracle as cx\n",
    "from flatten_json import flatten\n",
    "import time\n",
    "from multiprocessing import Process,Pool\n",
    "from  threading import  Thread\n",
    "import boto3 as bt\n",
    "import logging\n",
    "import logging_json as lg\n",
    "import jsonlines\n",
    "from functools import reduce\n",
    "import operator\n",
    "################################################################################################\n",
    "multikey = []\n",
    "casesProcessed = 0\n",
    "################################################################################################\n",
    "\n",
    "#! Fetching sqs from dump msg\n",
    "def recieve_message(url)->list:\n",
    "    lst = []\n",
    "    sqs_client = bt.client(\"sqs\",region_name=\"ap-south-1\")\n",
    "    response = sqs_client.receive_message(\n",
    "        QueueUrl = url,\n",
    "        MaxNumberOfMessages=3,\n",
    "        WaitTimeSeconds=0\n",
    "    )\n",
    "    val = response['Messages'][0]['Body']\n",
    "    val2 = json.loads(val)\n",
    "    bucket_name = val2['Records'][0]['s3']['bucket']['name']\n",
    "    fileKey = val2['Records'][0]['s3']['object']['key']\n",
    "    lst.append(bucket_name)\n",
    "    lst.append(fileKey)\n",
    "    return lst\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "def include_keys(dic, keys):\n",
    "    key_set = set(keys) & set(dic.keys())\n",
    "    return {key: dic[key] for key in key_set}\n",
    "\n",
    "#! Removing the ArrayElem\n",
    "def pre_proc(data):\n",
    "    blank_key_data = []\n",
    "    for k,v in data.items():    \n",
    "        if isinstance(v,dict):\n",
    "            if 'ArrayElem' in v.keys():\n",
    "                data[k]=v['ArrayElem']\n",
    "            if isinstance(data[k],list):\n",
    "                if len(data[k][0]) == 0:\n",
    "                    blank_key_data.append(k)\n",
    "    for key_to_be_deleted in blank_key_data:\n",
    "        del data[key_to_be_deleted]\n",
    "    # print(blank_key_data)\n",
    "    # print(data)\n",
    "    return data\n",
    "    \n",
    "    \n",
    "#! Flattening the dictionary \n",
    "def flatten_list(d):\n",
    "    try:\n",
    "        key, lst = next((k, v) for k, v in d.items() if isinstance(v, list))\n",
    "    except (StopIteration, AttributeError):\n",
    "        return [flatten(d,'.')]\n",
    "    return [flatten({**d, **{key: v}},'.') for record in lst for v in flatten_list(record)]\n",
    "\n",
    "\n",
    "def generate_inserts(table, data)->list:\n",
    "    jsons = flatten_list(data)\n",
    "    # print(jsons)\n",
    "    statements = []\n",
    "    for i in jsons:\n",
    "        cols = i.keys()\n",
    "        values = i.values()\n",
    "        ins = f\"INSERT INTO {table} ({','.join(cols)}) VALUES {tuple(values)};\".replace('None','null')\n",
    "        statements.append((ins))\n",
    "    return statements\n",
    "    \n",
    "\n",
    "# ! Creating insert statements\n",
    "def return_all_inserts(dictionary):\n",
    "    generic_keys = ['TENANT_ID','CASE_ID','VERSION_NUM']\n",
    "    inserts = {}\n",
    "    lst= []\n",
    "    dictionary2 = pre_proc(dictionary)\n",
    "    for i in dictionary2:\n",
    "        if i not in generic_keys:\n",
    "            t1 = include_keys(dictionary, generic_keys+[i])\n",
    "            for j in t1:\n",
    "                if isinstance(t1[j],dict):\n",
    "                    if 'ArrayElem' in t1[j].keys():\n",
    "                        t1[j] = t1[j]['ArrayElem']\n",
    "            # lst.append(tuple(generate_inserts(i,t1)))\n",
    "            lst.append(generate_inserts(i,t1))\n",
    "    with open('read.json','w') as f:\n",
    "        G = json.dumps(inserts)\n",
    "        f.write(G)\n",
    "    return reduce(operator.iconcat,lst,[])\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "def push_data(dictionary1, cnxn_pool)->None:\n",
    "    global failures\n",
    "    global casesProcessed\n",
    "    \n",
    "    cnxn = cnxn_pool.acquire()\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'DD-MM-YYYY hh24:mi:ss'\")\n",
    "    print(\"\"\"INCURSION :)(: STARTED\"\"\")\n",
    "    # failed_insert = {}\n",
    "    failed = False\n",
    "    # for INSERT  in dictionary2:\n",
    "        # for insert_statement in val:\n",
    "        #     try:\n",
    "        #         cursor.execute(f\"{insert_statement}\")\n",
    "        #     except cx.DatabaseError as e:\n",
    "        #         multikey.append({\"f_id\" :f\"{dictionary1['TENANT_ID']}^{dictionary1['CASE_ID']}^{dictionary1['VERSION_NUM']}\",\"tables\":f\"{key}-> {e}\"})\n",
    "        #         failed_insert[key] = val\n",
    "        #         # print(e)\n",
    "        #     # break\n",
    "        #     # else:\n",
    "        #         # print(\"Insert Done\")\n",
    "    insert = return_all_inserts(dictionary1)\n",
    "    try:\n",
    "        inserrt_block = '\\n'.join([\"BEGIN\"] + insert + [\"END;\"])\n",
    "        cursor.execute(inserrt_block)\n",
    "        \n",
    "    except cx.DatabaseError as e:\n",
    "        failed = True\n",
    "        print(e)\n",
    "            \n",
    "    # if len(failed_insert) == 0:\n",
    "    #     cnxn.commit()\n",
    "    #     casesProcessed+=1\n",
    "    #     print(\"All Inserts Committed\")\n",
    "    if failed == True:\n",
    "        cnxn.rollback()\n",
    "        \n",
    "    # else:\n",
    "    #     cnxn.commit()\n",
    "        # failures.append(failed_insert)\n",
    "        # print(\"Rollback Done\")\n",
    "        # print(f\"Number of cases processed -> : {casesProcessed}\")\n",
    "    print(\"Failed or not->\",{failed})\n",
    "    cnxn_pool.release(cnxn)\n",
    "    \n",
    "    \n",
    "################################################################################################\n",
    "\n",
    "# ! Splitting the file into multiple chunk_data \n",
    "def split_file(a,n)->list:\n",
    "    k,m = divmod(len(a),n)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)))\n",
    "\n",
    "def load(file)->list:\n",
    "    with open(file,'r') as fp:\n",
    "        string = fp.read()\n",
    "        o = json.loads(string)\n",
    "        line_chunk = split_file(o,1)\n",
    "        print('chunk created')\n",
    "        return line_chunk\n",
    "# Set the NLS_DATE_FORMAT for a session\n",
    "# def init_session(connection, requested_tag):\n",
    "#     cursor = connection.cursor()\n",
    "#     # cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'DD-MM-YYYY hh24:mi:ss'\")\n",
    "if __name__ == \"__main__\":\n",
    "    st = time.time()\n",
    "    # try:\n",
    "    #     arr = recieve_message(\"https://sqs.ap-south-1.amazonaws.com/884379823401/testQueue\")\n",
    "    #     s3 = bt.client('s3')\n",
    "    #     s3.download_file(Bucket=arr[0],Key=arr[1],Filename='C:/DEV/PY_DEV/output.json')\n",
    "    # except Exception as e:\n",
    "    #     logging.error(\"Error downloading file\")\n",
    "    failures = []\n",
    "    cnxn_pool = cx.SessionPool('PVD_MART_30AUG','rxlogix','10.100.22.99:1521/PVSDEVDB',min=1,max=1,getmode=cx.SPOOL_ATTRVAL_WAIT,encoding='UTF-8')\n",
    "    #  getmode value can be set so that any acquire() call will wait for a connection to become available if all are currently in use\n",
    "    chunk_data = load('jsons/msg3.json')\n",
    "    threads = []\n",
    "    for i in chunk_data[0]:\n",
    "        thread = Thread(target=push_data, args=(i,cnxn_pool),daemon=True)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    # push_data(chunk_data[0],cnxn_pool)\n",
    "    # cnxn_pool.close()\n",
    "    #Uploading Failed Inserts to S3\n",
    "    # with open('jsons/failues.json', 'w') as F:\n",
    "    #     J = json.dumps(failures, encoding = 'utf-8',indent =1)\n",
    "    #     F.write(J)\n",
    "    #     s3.upload_file(F,Bucket=arr[0])\n",
    "    with open('logs.json','w',encoding='utf-8') as logs:\n",
    "                G = json.dumps(multikey,indent=1)\n",
    "                logs.write(G)\n",
    "    print(f\" Number of Failed Cases : {len(failures)}\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b044e500be76e278f8af992ff09c18d106167691629593a72c8d65315c33d3bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
