{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ? insert stat dump\n",
    "import json\n",
    "import cx_Oracle as cx\n",
    "from flatten_json import flatten\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "cnxn_pool = cx.SessionPool('PVD_MART_30AUG','rxlogix','10.100.22.99:1521/PVSDEVDB',min=12,max=15,encoding='UTF-8')\n",
    "\n",
    "def include_keys(dic, keys):\n",
    "    key_set = set(keys) & set(dic.keys())\n",
    "    return {key: dic[key] for key in key_set}\n",
    "\n",
    "def pre_proc(data):\n",
    "        print(type(data))\n",
    "        for k,v in data.items():\n",
    "            if isinstance(v,dict):\n",
    "                if 'ArrayElem' in v.keys():\n",
    "                # for k1,v1 in v.items():\n",
    "                    data[k]=v['ArrayElem']\n",
    "        return data\n",
    "    \n",
    "def flatten_list(d):\n",
    "    try:\n",
    "        key, lst = next((k, v) for k, v in d.items() if isinstance(v, list))\n",
    "    except (StopIteration, AttributeError):\n",
    "        return [flatten(d,'.')]\n",
    "    return [flatten({**d, **{key: v}},'.') for record in lst for v in flatten_list(record)]\n",
    "\n",
    "def generate_inserts(table, data):\n",
    "    jsons = flatten_list(data)\n",
    "    statements = []\n",
    "    ins = jsons[0]\n",
    "    ins = f\"INSERT INTO {table} ({','.join(ins.keys())}) VALUES ({','.join([':'+str(v) for v in range(len(ins.keys()))])})\"\n",
    "    val_array = []\n",
    "    for i in jsons:\n",
    "        cols = i.keys()\n",
    "        values = i.values()\n",
    "        # ins = f\"INSERT INTO {table} ({','.join(cols)}) VALUES ({','.join(['%s']*len(cols))})\"\n",
    "        val_array.append(tuple(values))\n",
    "    statements.append(ins)\n",
    "    statements.append(val_array)\n",
    "    return statements\n",
    "\n",
    "def return_all_inserts(dictionary)->dict:\n",
    "    generic_keys = ['TENANT_ID','CASE_ID','VERSION_NUM']\n",
    "    inserts = {}\n",
    "    dictionary2 = pre_proc(dictionary)\n",
    "    for i in dictionary2:\n",
    "        if i not in generic_keys:\n",
    "            t1 = include_keys(dictionary, generic_keys+[i])\n",
    "            for j in t1:\n",
    "                if isinstance(t1[j],dict):\n",
    "                    if 'ArrayElem' in t1[j].keys():\n",
    "                        t1[j] = t1[j]['ArrayElem']\n",
    "            inserts[i] = tuple(generate_inserts(i,t1))\n",
    "    return inserts\n",
    "\n",
    "\n",
    "def push_data(dictionary1):\n",
    "    print(\"Thread created\")\n",
    "    dictionary2 = return_all_inserts(dictionary1)\n",
    "    cnxn = cnxn_pool.acquire()\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'DD/MM/YYYY hh24:mi:ss'\")\n",
    "    print(\"\"\"INCURSION :)(: STARTED\"\"\")\n",
    "    counter = 1\n",
    "    failed_insert = {}\n",
    "    for key,val  in dictionary2.items(): \n",
    "        val_array = val[1]\n",
    "        # print(val[0],val_array)\n",
    "        try:\n",
    "            cursor.executemany(val[0],val_array)\n",
    "            # cnxn.commit()\n",
    "        except cx.DatabaseError as e:\n",
    "            print(e)\n",
    "            failed_insert[key] = val\n",
    "            # break\n",
    "        # else:\n",
    "            # print(\"Insert Done\")\n",
    "        # print(counter,key)\n",
    "        counter+=1\n",
    "    if len(failed_insert) == 0:\n",
    "        cnxn.commit()\n",
    "        print(\"All Inserts Committed\")\n",
    "    else:\n",
    "        cnxn.rollback()\n",
    "        print(\"Rollback Done\")\n",
    "        # print(failed_insert)\n",
    "    print(\"data chunk inserted\")\n",
    "    cnxn_pool.release(cnxn)\n",
    "    \n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "#     with open('jsons/msg1.json','r') as msg1:\n",
    "#         string = msg1.read()\n",
    "#         D = json.loads(string)\n",
    "#         # dictionary = return_all_inserts(D)\n",
    "#         push_data(D)\n",
    "#     cnxn_pool.close()\n",
    "    \n",
    "def split_file(a,n):\n",
    "    k,m = divmod(len(a),n)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)))\n",
    "\n",
    "def load(file)->list:\n",
    "    with open(file,'r', encoding=\"utf8\") as fp:\n",
    "        string = fp.read()\n",
    "        o = json.loads(string)\n",
    "        line_chunk = split_file(o,1)\n",
    "        print('chunk created')\n",
    "        return line_chunk\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    st = time.time()\n",
    "    chunk_data = load('jsons/msg1.json')\n",
    "    with ThreadPoolExecutor(max_workers=10) as threads:\n",
    "        threads.map(push_data,chunk_data)\n",
    "    cnxn_pool.close()\n",
    "    et = time.time()\n",
    "    print(et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ? trunacting the table\n",
    "import json\n",
    "import cx_Oracle as cx\n",
    "cdns = cx.makedsn('10.100.22.99','1521',service_name='PVSDEVDB')\n",
    "cnxn = cx.connect(user=r'PVD_MART_30AUG', password='rxlogix', dsn=cdns)\n",
    "c = cnxn.cursor()\n",
    "with open('read.json','r') as insert_stat:\n",
    "        string = insert_stat.read()\n",
    "        X = json.loads(string)\n",
    "        arr = []\n",
    "        for key,val  in X.items():\n",
    "            arr.append(key,)\n",
    "            c.execute(f\"TRUNCATE TABLE {key}\")\n",
    "        print(\"truncate complete\")\n",
    "cnxn.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#? o/p of select statements\n",
    "\n",
    "cdns = cx.makedsn('10.100.22.99','1521',service_name='PVSDEVDB')\n",
    "cnxn = cx.connect(user=r'PVD_MART_30AUG', password='rxlogix', dsn=cdns)\n",
    "c = cnxn.cursor()\n",
    "with open('read.json','r') as insert_stat:\n",
    "        string = insert_stat.read()\n",
    "        X = json.loads(string)\n",
    "        arr = []\n",
    "        for key,val  in X.items():\n",
    "            arr.append(key)\n",
    "        print(arr)\n",
    "        for key in arr:\n",
    "            c.execute(f\"TRUNCATE TABLE {key}\")\n",
    "            res = c.fetchall()\n",
    "            for i in res:\n",
    "                print(i)\n",
    "            print(key)\n",
    "cnxn.close()\n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? insert stat dump\n",
    "import json\n",
    "import cx_Oracle as cx\n",
    "from flatten_json import flatten\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from multiprocessing import Process,Pool\n",
    "from  threading import Lock\n",
    "cnxn_pool = cx.SessionPool('PVD_MART_30AUG','rxlogix','10.100.22.99:1521/PVSDEVDB',min=12,max=15,encoding='UTF-8')\n",
    "\n",
    "def include_keys(dic, keys):\n",
    "    key_set = set(keys) & set(dic.keys())\n",
    "    return {key: dic[key] for key in key_set}\n",
    "\n",
    "def pre_proc(data):\n",
    "        print(type(data))\n",
    "        for k,v in data.items():\n",
    "            if isinstance(v,dict):\n",
    "                if 'ArrayElem' in v.keys():\n",
    "                # for k1,v1 in v.items():\n",
    "                    data[k]=v['ArrayElem']\n",
    "        return data\n",
    "    \n",
    "def flatten_list(d):\n",
    "    try:\n",
    "        key, lst = next((k, v) for k, v in d.items() if isinstance(v, list))\n",
    "    except (StopIteration, AttributeError):\n",
    "        return [flatten(d,'.')]\n",
    "    return [flatten({**d, **{key: v}},'.') for record in lst for v in flatten_list(record)]\n",
    "\n",
    "def generate_inserts(table, data):\n",
    "    jsons = flatten_list(data)\n",
    "    statements = []\n",
    "    ins = jsons[0]\n",
    "    ins = f\"INSERT INTO {table} ({','.join(ins.keys())}) VALUES ({','.join([':'+str(v) for v in range(len(ins.keys()))])})\"\n",
    "    val_array = []\n",
    "    for i in jsons:\n",
    "        cols = i.keys()\n",
    "        values = i.values()\n",
    "        # ins = f\"INSERT INTO {table} ({','.join(cols)}) VALUES ({','.join(['%s']*len(cols))})\"\n",
    "        val_array.append(tuple(values))\n",
    "    statements.append(ins)\n",
    "    statements.append(val_array)\n",
    "    return statements\n",
    "\n",
    "def return_all_inserts(dictionary)->dict:\n",
    "    generic_keys = ['TENANT_ID','CASE_ID','VERSION_NUM']\n",
    "    inserts = {}\n",
    "    dictionary2 = pre_proc(dictionary)\n",
    "    for i in dictionary2:\n",
    "        if i not in generic_keys:\n",
    "            t1 = include_keys(dictionary, generic_keys+[i])\n",
    "            for j in t1:\n",
    "                if isinstance(t1[j],dict):\n",
    "                    if 'ArrayElem' in t1[j].keys():\n",
    "                        t1[j] = t1[j]['ArrayElem']\n",
    "            inserts[i] = tuple(generate_inserts(i,t1))\n",
    "    return inserts\n",
    "\n",
    "\n",
    "def push_data(dictionary1):\n",
    "    lock.acquire()\n",
    "    print(\"Thread created\")\n",
    "    dictionary2 = return_all_inserts(dictionary1[0])\n",
    "    # print(dictionary2)\n",
    "    cnxn = cnxn_pool.acquire()\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.execute(\"ALTER SESSION SET NLS_DATE_FORMAT = 'DD/MM/YYYY hh24:mi:ss'\")\n",
    "    print(\"\"\"INCURSION :)(: STARTED\"\"\")\n",
    "    counter = 1\n",
    "    failed_insert = {}\n",
    "    for key,val  in dictionary2.items(): \n",
    "        val_array = val[1]\n",
    "        # print(val[0],val_array)\n",
    "        try:\n",
    "            cursor.executemany(val[0],val_array)\n",
    "            # cnxn.commit()\n",
    "        except cx.DatabaseError as e:\n",
    "            print(e)\n",
    "            failed_insert[key] = val\n",
    "            # break\n",
    "        # else:\n",
    "            # print(\"Insert Done\")\n",
    "        # print(counter,key)\n",
    "        counter+=1\n",
    "    if len(failed_insert) == 0:\n",
    "        cnxn.commit()\n",
    "        print(\"All Inserts Committed\")\n",
    "    else:\n",
    "        cnxn.rollback()\n",
    "        print(\"Rollback Done\")\n",
    "        # print(failed_insert)\n",
    "    print(\"data chunk inserted\")\n",
    "    cnxn_pool.release(cnxn)\n",
    "    lock.release()\n",
    "    # failed_insert\n",
    "    \n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "#     with open('jsons/msg1.json','r') as msg1:\n",
    "#         string = msg1.read()\n",
    "#         D = json.loads(string)\n",
    "#         # dictionary = return_all_inserts(D)\n",
    "#         push_data(D)\n",
    "#     cnxn_pool.close()\n",
    "    \n",
    "def split_file(a,n):\n",
    "    k,m = divmod(len(a),n)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)))\n",
    "\n",
    "def load(file)->list:\n",
    "    with open(file,'r', encoding=\"utf8\") as fp:\n",
    "        string = fp.read()\n",
    "        o = json.loads(string)\n",
    "        line_chunk = split_file(o,1)\n",
    "        print('chunk created')\n",
    "        return line_chunk\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    st = time.time()\n",
    "    chunk_data = load('jsons/msg1.json')\n",
    "    lock = Lock()\n",
    "    with ThreadPoolExecutor(max_workers=10) as threads:\n",
    "        threads.map(push_data,chunk_data,lock)\n",
    "    cnxn_pool.close()\n",
    "    # # chunk_data = load('output.json')\n",
    "    # pool = Pool(7)\n",
    "    # pool.map(push_data,chunk_data)\n",
    "    # cnxn_pool.close()\n",
    "    et = time.time()\n",
    "    print(et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def split_file(a,n):\n",
    "    k,m = divmod(len(a),n)\n",
    "    return list((a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)))\n",
    "\n",
    "def load(file)->list:\n",
    "    with open(file,'r', encoding=\"utf8\") as fp:\n",
    "        string = fp.read()\n",
    "        o = json.loads(string)\n",
    "        line_chunk = split_file(o,1)\n",
    "        print('chunk created')\n",
    "        return line_chunk\n",
    "    \n",
    "chunk_data = load('jsons/msg1.json')\n",
    "print(len(chunk_data[0]))\n",
    "print(chunk_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b044e500be76e278f8af992ff09c18d106167691629593a72c8d65315c33d3bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
